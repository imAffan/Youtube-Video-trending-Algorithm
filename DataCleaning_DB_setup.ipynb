{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from config import dbuser, dbpassword, dbhost, dbport, dbname\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING A DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'video_id', 'title', 'publishedAt', 'channelTitle',\n",
       "       'categoryId', 'trending_date', 'views', 'likes', 'dislikes', 'comments',\n",
       "       'thumbnail_link'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================== #\n",
    "# # # CREATING A HEROKU DATASET\n",
    "# =============================================================== #\n",
    "# Heroku only takes data with <10k rows so I created a heroku dataset (incase we ever want to host on heroku).\n",
    "\n",
    "def import_func(country_code):\n",
    "    \n",
    "    # Creating Path\n",
    "    path = os.path.join('data','newData',f'{country_code}_youtube_trending_data.csv')\n",
    "    \n",
    "    # Storing dataframe to df\n",
    "    dfh=pd.read_csv(path)\n",
    "    \n",
    "    # Removing unwanted columns below\n",
    "    dfh=dfh[['video_id','title','publishedAt','channelTitle','categoryId','trending_date','view_count','likes','dislikes','comment_count','thumbnail_link']]\n",
    "    \n",
    "    # Renaming columns\n",
    "    dfh = dfh.rename(columns={'view_count': 'views', 'likes': 'likes', 'dislikes': 'dislikes', 'comment_count': 'comments'})\n",
    "    \n",
    "    # Changing object types to date types for two columns\n",
    "    dfh['publishedAt']=pd.to_datetime(dfh['publishedAt'])\n",
    "    dfh['trending_date']=pd.to_datetime(dfh['trending_date'])\n",
    "    \n",
    "    # Removing time stamp from date\n",
    "    dfh['publishedAt']=dfh['publishedAt'].dt.date \n",
    "    dfh['trending_date']=dfh['trending_date'].dt.date\n",
    "    \n",
    "    # For loop for each csv file\n",
    "    with open(f'data/newData/{country_code}_category_id.json', 'r') as read_file:\n",
    "        category_ids = json.load(read_file)\n",
    "\n",
    "        dfh=dfh.astype({'categoryId': 'str'})\n",
    "        \n",
    "    for index,row in dfh.iterrows():\n",
    "    \n",
    "        for entry in category_ids[\"items\"]:\n",
    "\n",
    "            if row[\"categoryId\"]==entry[\"id\"]:\n",
    "                dfh.at[index,\"categoryId\"]=entry[\"snippet\"][\"title\"]\n",
    "    \n",
    "    # Select 999 rows from each country in dataframe (so we end up with <10k rows ~1000 rows per country * 10 countries)\n",
    "    # dfh = dfh.nlargest(999, 'views')\n",
    "    \n",
    "    # Adding country Code as column\n",
    "    dfh['country']=f'{country_code}'\n",
    "    col_name='country'\n",
    "    \n",
    "    # Moving country code to first column\n",
    "    first_col = dfh.pop(col_name)\n",
    "    dfh.insert(0,col_name,first_col)\n",
    "    \n",
    "    return dfh\n",
    "\n",
    "# USA Dataframe\n",
    "dfh_us=import_func('US')\n",
    "\n",
    "# Brasil Dataframe\n",
    "dfh_br=import_func('BR')\n",
    "\n",
    "# Canada Dataframe\n",
    "dfh_ca=import_func('CA')\n",
    "\n",
    "# Mexico Dataframe\n",
    "dfh_mx=import_func('MX')\n",
    "\n",
    "# GB Dataframe\n",
    "dfh_gb=import_func('GB')\n",
    "\n",
    "# France Dataframe\n",
    "dfh_fr=import_func('FR')\n",
    "\n",
    "# Russia Dataframe\n",
    "dfh_ru=import_func('RU')\n",
    "\n",
    "# Japan Dataframe\n",
    "dfh_jp=import_func('JP')\n",
    "\n",
    "# Korea Dataframe\n",
    "dfh_kr=import_func('KR')\n",
    "\n",
    "# India Dataframe\n",
    "dfh_in=import_func('IN')\n",
    "\n",
    "\n",
    "# Creating a varible to add all dfs\n",
    "country_dfh=[dfh_us, dfh_br, dfh_ca, dfh_mx, dfh_gb, dfh_fr, dfh_ru ,dfh_jp, dfh_kr ,dfh_in]\n",
    "\n",
    "# Merge output into one table\n",
    "dfh_main = pd.concat(country_dfh)\n",
    "dfh_main\n",
    "\n",
    "# Fix the 29 vs Non profits issue\n",
    "dfh_main['categoryId'] = dfh_main['categoryId'].replace([\"29\"],\"Nonprofits & Activism\")\n",
    "\n",
    "# View output\n",
    "dfh_main.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View output\n",
    "dfh_main\n",
    "dfh_main.to_csv(r'C:\\Users\\mohan\\Downloads\\CsvDownloadtest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONNECTING AND LOADING DATA INTO THE DATABASE (POSTGRES & MSSQL DBs) FOR HEROKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================== #\n",
    "# # UPLOAD DATASET TO DATABASE (POSTGRESS SQL DB)\n",
    "# =============================================================== #\n",
    "# You first have to go create the db and then use the same name as the db you have created\n",
    "# (youtube_table_v1 < 10,000    youtube_table_v2 > 10,000) in postgres\n",
    "\n",
    "# =============================================================== #\n",
    "# # 1 Connect to Database (with Local db and all)\n",
    "# =============================================================== #\n",
    "pg_user = 'postgres'\n",
    "pg_password = 'Sm6Jc5bqbiNQdsVAo7eN'\n",
    "db_name = 'YTP_database'\n",
    "connection_string = f'{pg_user}:{pg_password}@localhost:5432/{db_name}'\n",
    "engine=create_engine(f'postgresql://{connection_string}')\n",
    "\n",
    "# =============================================================== #\n",
    "# 2 Connect to Database (Alternative with AWS db and all)\n",
    "# =============================================================== #\n",
    "# dbuser = 'postgres'\n",
    "# dbpassword = 'Sm6Jc5bqbiNQdsVAo7eN'\n",
    "# dbhost = 'localhost'\n",
    "# dbport = '5432'\n",
    "# dbname= 'YTP_database'\n",
    "# connection_string2 = f'{dbuser}:{dbpassword}@database-1.cvmfiiilpm7y.us-east-1.rds.amazonaws.com:{dbport}/{dbname}'\n",
    "# engine=create_engine(f'postgresql://{connection_string2}')\n",
    "\n",
    "# =============================================================== #\n",
    "# 3 Upload to postgres\n",
    "# =============================================================== #\n",
    "dfh_main.to_sql(name='youtube_table_v2',con=engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================== #\n",
    "# # UPLOAD DATASET TO DATABASE (SQL SERVER DB)\n",
    "# =============================================================== #\n",
    "# =============================================================== #\n",
    "# IMPORTING DEPENDENCIES\n",
    "# =============================================================== #\n",
    "# from sqlalchemy import create_engine\n",
    "# import urllib\n",
    "\n",
    "# =============================================================== #\n",
    "# Set up variables\n",
    "# =============================================================== #\n",
    "# msql_serverName = 'WORKHORSE_PC\\SQLEXPRESS'\n",
    "# msql_dbName = 'YTP_Database'\n",
    "\n",
    "# =============================================================== #\n",
    "# Set up connection string (This connection string is for SQL \n",
    "# server which are set up without username & pw)\n",
    "# =============================================================== #\n",
    "\n",
    "# conn_str = (\n",
    "#     r'Driver=ODBC Driver 17 for SQL Server;'\n",
    "#     rf'Server={msql_serverName};'\n",
    "#     rf'Database={msql_dbName};'\n",
    "#     r'Trusted_Connection=yes;'\n",
    "# )\n",
    "# quoted_conn_str = urllib.parse.quote_plus(conn_str)\n",
    "# engine = create_engine(f'mssql+pyodbc:///?odbc_connect={quoted_conn_str}')\n",
    "# cnxn = engine.connect()\n",
    "\n",
    "# =============================================================== #\n",
    "# Connect to DB and delete (or create) a table\n",
    "# =============================================================== #\n",
    "\n",
    "# q = \"\"\"DROP TABLE youtube_table_v2\"\"\"\n",
    "# cnxn.execute(q)   \n",
    "\n",
    "# q = \"\"\" \n",
    "# CREATE TABLE youtube_table_v2 (\n",
    "# country Varchar (128) NOT NULL,\n",
    "# video_id Varchar (128) NOT NULL,\n",
    "# title Varchar (128) NOT NULL,\n",
    "# publishedAt datetime NOT NULL,\n",
    "# channelTitle Varchar (128),\n",
    "# categoryId Varchar (128) NOT NULL,\n",
    "# trending_date datetime NOT NULL,\n",
    "# views Integer NOT NULL,\n",
    "# likes Integer NOT NULL,\n",
    "# dislikes Integer NOT NULL,\n",
    "# comments Integer NOT NULL,\n",
    "# thumbnail_link Varchar (128) NOT NULL,\n",
    "# )\n",
    "# \"\"\"\n",
    "\n",
    "# cnxn.execute(q)  \n",
    "\n",
    "# =============================================================== #\n",
    "# Connect to DB and upload df to table\n",
    "# =============================================================== #\n",
    "\n",
    "# cnxn = engine.connect()\n",
    "# dfh_main.to_sql(name = 'youtube_table_v2', con = cnxn, if_exists = 'append',index = False)\n",
    "# cnxn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a403d1d0a703b42598ee278186dc3ccc27f0fdafd1d40ee100c32270c9393335"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
